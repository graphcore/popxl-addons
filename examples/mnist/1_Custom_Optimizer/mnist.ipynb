{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c215bbdd",
   "metadata": {},
   "source": [
    "# Custom Optimizer\n",
    "We have seen in the [previous notebook]() that we can easily create graphs with an internal state using the ```addons.Module``` class. \n",
    "Besides layers, also optimizers often carry a state. Hence, they are a perfect use case for the ```Module``` class.\n",
    "\n",
    "In this notebook we will show how to create a custom optimizer with state, the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46255db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "from typing import Mapping, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "import popxl\n",
    "import popxl_addons as addons\n",
    "import popxl.ops as ops\n",
    "from typing import Union, Dict\n",
    "from popxl_addons.graph import GraphWithNamedArgs\n",
    "from popxl_addons.named_tensors import NamedTensors\n",
    "from popxl_addons.input_factory import NamedInputFactories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torchvision\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4634cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adam optimizer.\n",
    "Defines adam update step for a single variable\n",
    "'''\n",
    "class Adam(addons.Module):\n",
    "    # we need to specify in_sequence because a lot of operations are in place and their order \n",
    "    # shouldn't be rearranged \n",
    "    @popxl.in_sequence()\n",
    "    def build(self,\n",
    "              var: popxl.TensorByRef,\n",
    "              grad: popxl.Tensor,\n",
    "              *,\n",
    "              lr: Union[float, popxl.Tensor],\n",
    "              beta1: Union[float, popxl.Tensor] = 0.9,\n",
    "              beta2: Union[float, popxl.Tensor] = 0.999,\n",
    "              eps: Union[float, popxl.Tensor] = 1e-5,\n",
    "              weight_decay: Union[float, popxl.Tensor] = 1e-2,\n",
    "              first_order_dtype: popxl.dtype = popxl.float16,\n",
    "              bias_correction: bool = True):\n",
    "\n",
    "        # gradient estimators for the variable var - same shape as the variable \n",
    "        first_order = self.add_input_tensor(\"first_order\", partial(np.zeros, var.shape), first_order_dtype, by_ref=True)\n",
    "        ops.var_updates.accumulate_moving_average_(first_order, grad, f=beta1)\n",
    "\n",
    "        # variance estimators for the variable var - same shape as the variable \n",
    "        second_order = self.add_input_tensor(\"second_order\", partial(np.zeros, var.shape), popxl.float32, by_ref=True)\n",
    "        ops.var_updates.accumulate_moving_average_square_(second_order, grad, f=beta2)\n",
    "\n",
    "        # adam is a biased estimator: provide the step to correct bias\n",
    "        step = None\n",
    "        if bias_correction:\n",
    "            step = self.add_input_tensor(\"step\", partial(np.zeros, ()), popxl.float32, by_ref=True)\n",
    "\n",
    "        # calculate the weight increment with adam euristic \n",
    "        updater = ops.var_updates.adam_updater(\n",
    "            first_order, second_order,\n",
    "            weight=var,\n",
    "            weight_decay=weight_decay,\n",
    "            time_step=step,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            epsilon=eps)\n",
    "\n",
    "        # in place weight update: w += (-lr)*dw\n",
    "        ops.scaled_add_(var, updater, b=-lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc12f2",
   "metadata": {},
   "source": [
    "The first important thing to note is that all the build method is executed **in sequence**, as we've added the ```@popxl.in_sequence()``` decorator. \n",
    "It is necessary since most of the optimizer operations are **in place**, hence their order of execution must be stricly preserved.\n",
    "\n",
    "Note also that the ```var``` input is a ```popxl.TensorByRef```: any change made to this variable will be automatically copied to the parent graph. See [TensorByRef]() for more information.\n",
    "\n",
    "Allowing a ```Union[float, popxl.Tensor]``` type for the optimizer parameters such as the learning rate or the weight decay gives this module an interesting property.\n",
    "If the parameter is provided as a ```float```, it will be \"baked\" into the graph, with no possibility of changing it.\n",
    "Instead, if the parameter is a ```Tensor``` (or ```TensorSpec```) it will appear as an input to the graph, which needs to be provided when calling the graph. If you plan to change a parameter (for example, because you have a learning rate schedule), this is the way to go.\n",
    "\n",
    "The rest of the logic is straightforward:\n",
    "\n",
    "- we update the first moment, estimator for the gradient of the variable\n",
    "- we update the second moment, estimator for the variance of the variable\n",
    "- we optionally correct the estimators, since they are biased\n",
    "- we compute the increment for the variable, ```dw```\n",
    "- we update the variable ``` w += (-lr) * dw```\n",
    "\n",
    "The ```ops.var_updates``` module contains several useful pre-made update rules, but you can also make your own. In this example we are using three of them:\n",
    "\n",
    "- ```ops.var_updates.accumulate_moving_average_(average, new_sample, coefficient)``` updates ```average``` in place with an exponential moving average rule: \n",
    "    ```\n",
    "    average = (coefficient * average) + ((1-coefficient) * new_sample)\n",
    "    ```\n",
    "- ```accumulate_moving_average_square_(average, new_sample, coefficient)``` does the same, but using the square of the sample.  \n",
    "- ```ops.var_updates.adam_updater(...)``` returns the adam increment ```dw``` which is required for the weight update, computed using adam internal state, i.e. the first and second moments.\n",
    "\n",
    "Let's inspect the optimizer graph and its use in a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ffef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = popxl.Ir()\n",
    "ir.replication_factor = 1 \n",
    "\n",
    "with ir.main_graph:\n",
    "    var = popxl.variable(np.ones((2,2)),popxl.float32)\n",
    "    grad = popxl.variable(np.full((2,2),0.1),popxl.float32)\n",
    "    # create graph and factories - float learning rate\n",
    "    adam_facts, adam = Adam(cache=True).create_graph(var, var.spec, lr=1e-3)\n",
    "    # create graph and factories - Tensor learning rate\n",
    "    adam_facts_lr, adam_lr = Adam().create_graph(var, var.spec, lr=popxl.TensorSpec((),popxl.float32))\n",
    "    print(\"Adam with float learning rate\\n\")\n",
    "    print(adam.print_schedule())\n",
    "    print(\"\\n Adam with tensor learning rate\\n\")\n",
    "    print(adam_lr.print_schedule())\n",
    "    # instantiate optimizer variables \n",
    "    adam_state = adam_facts.init()\n",
    "    adam_state_lr = adam_facts_lr.init()\n",
    "    # optimization step for float lr: call the bound graph providing the variable to update and the gradient \n",
    "    adam.bind(adam_state).call(var, grad)\n",
    "    # optimization step for tensor lr: call the bound graph providing the variable to update, the gradient and the learning rate\n",
    "    adam_lr.bind(adam_state_lr).call(var, grad, popxl.constant(1e-3))\n",
    "\n",
    "ir.num_host_transfers = 1\n",
    "session = popxl.Session(ir,\"ipu_hw\")\n",
    "print(\"\\n Before adam update\")\n",
    "var_data = session.get_tensor_data(var)\n",
    "state = session.get_tensors_data(adam_state.tensors)\n",
    "print(\"Variable:\\n\", var)\n",
    "print(\"Adam state:\")\n",
    "for name, data in state.items():\n",
    "    print(name,'\\n', state[name])\n",
    "\n",
    "session.run()\n",
    "\n",
    "print(\"\\n After adam update\")\n",
    "var_data = session.get_tensor_data(var)\n",
    "state = session.get_tensors_data(adam_state.tensors)\n",
    "print(\"Variable:\\n\", var)\n",
    "print(\"Adam state:\")\n",
    "for name, data in state.items():\n",
    "    print(name,'\\n', state[name])\n",
    "\n",
    "session.device.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e952f",
   "metadata": {},
   "source": [
    "# Mnist with Adam\n",
    "We can now refactor our mnist example to incorporate the Adam optimizer. \n",
    "Note that we need an optimizer for each variable: we first define a utility function to create all the graphs and perform a full weight update for all the variables in the neural network. \n",
    "\n",
    "We will use a float learning rate, since we don't plan to change its value during training.\n",
    "\n",
    "The training code is almost unchanged from that of the previous tutorial, the only different piece is the code related to the optimizer in  ```train_program```. Also, since we are using Adam, we need to use a smaller learning rate. \n",
    "\n",
    "You will notice that we create the Adam module using \n",
    "```python\n",
    "optimizer = Adam(cache=True)\n",
    "```\n",
    "Using `cache=True` will enable graph reuse, if possible, when calling `optimizer.create_graph`. For our optimizer this would be when there are multiple variables with the same shape/dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Update all variables creating per-variable optimizers. \n",
    "'''\n",
    "def optimizer_step(variables,\n",
    "                   grads: Dict[popxl.Tensor, popxl.Tensor],\n",
    "                   optimizer : addons.Module,\n",
    "                   lr : popxl.float32 = 1e-3):\n",
    "    for name, var in variables.named_tensors.items():\n",
    "        #create optimizer and state factories for the variable\n",
    "        opt_facts, opt_graph = optimizer.create_graph(\n",
    "            var,\n",
    "            var.spec,\n",
    "            lr=lr, \n",
    "            weight_decay=0.0,\n",
    "            bias_correction=False\n",
    "            )\n",
    "        state = opt_facts.init()\n",
    "        # bind the graph to its state and call it.\n",
    "        # Both the state and the variables are updated in place and are passed by ref,\n",
    "        # hence after the graph is called they are updated.\n",
    "        opt_graph.bind(state).call(var, grads[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc73c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(test_batch_size: int, batch_size: int):\n",
    "    training_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            '~/.torch/datasets',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307, ), (0.3081, )),  # mean and std computed on the training set.\n",
    "            ])),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(torchvision.datasets.MNIST('~/.torch/datasets',\n",
    "                                                                             train=False,\n",
    "                                                                             download=True,\n",
    "                                                                             transform=torchvision.transforms.Compose([\n",
    "                                                                                 torchvision.transforms.ToTensor(),\n",
    "                                                                                 torchvision.transforms.Normalize(\n",
    "                                                                                     (0.1307, ), (0.3081, )),\n",
    "                                                                             ])),\n",
    "                                                  batch_size=test_batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  drop_last=True)\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0\n",
    "\n",
    "\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_input_tensor(\"weight\", partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "                                  x.dtype)\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_input_tensor(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class Net(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512)\n",
    "        self.fc2 = Linear(512)\n",
    "        self.fc3 = Linear(512)\n",
    "        self.fc4 = Linear(10)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "        x = ops.gelu(self.fc2(x))\n",
    "        x = ops.gelu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe68eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "test_batch_size = 80\n",
    "device = \"ipu_hw\" \n",
    "lr = 1e-3\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13943e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_program(batch_size, device, lr):\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = 1\n",
    "    with ir.main_graph:\n",
    "        # Create input streams from host to device \n",
    "        img_stream = popxl.h2d_stream((batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        img_t = ops.host_load(img_stream) #load data\n",
    "        label_stream = popxl.h2d_stream((batch_size, ), popxl.int32, \"labels\")\n",
    "        labels = ops.host_load(label_stream, \"labels\")\n",
    "\n",
    "        # Create forward graph\n",
    "        facts, fwd_graph = Net().create_graph(img_t)\n",
    "        # Create backward graph via autodiff transform\n",
    "        bwd_graph = addons.autodiff(fwd_graph)\n",
    "\n",
    "        # Initialise variables (weights)\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Call the forward with call_with_info because we want to retrieve information from the call site\n",
    "        fwd_info = fwd_graph.bind(variables).call_with_info(img_t)\n",
    "        x = fwd_info.outputs[0] # forward output\n",
    "        \n",
    "        # Compute loss and starting gradient for backprop \n",
    "        loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "        \n",
    "        # Setup a stream to retrieve loss values from the host\n",
    "        loss_stream = popxl.d2h_stream(loss.shape, loss.dtype, \"loss\")\n",
    "        ops.host_store(loss_stream, loss)\n",
    "        \n",
    "        # retrieve activations from the forward\n",
    "        activations = bwd_graph.grad_graph_info.inputs_dict(fwd_info)\n",
    "        # call the backward providing the starting value for backprop and activations\n",
    "        bwd_info = bwd_graph.call_with_info(dx, args=activations)\n",
    "        \n",
    "        # Adam Optimizer, with cache\n",
    "        grads_dict = bwd_graph.grad_graph_info.fwd_parent_ins_to_grad_parent_outs(fwd_info, bwd_info)\n",
    "        optimizer = Adam(cache=True)\n",
    "        optimizer_step(variables, grads_dict, optimizer, lr)\n",
    "            \n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir,device), [img_stream, label_stream], variables, loss_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = get_mnist_data(test_batch_size, train_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(train_batch_size,device,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_batches = len(training_data)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"Epoch {0}/{1}\".format(epoch, epochs))\n",
    "    bar = tqdm(training_data, total=nr_batches)\n",
    "    for data, labels in bar:\n",
    "        inputs : Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(zip(train_input_streams,\n",
    "                                                                        [data.squeeze().float(),\n",
    "                                                                         labels.int()]))\n",
    "        loss = train_session.run(inputs)[loss_stream]\n",
    "        bar.set_description(\"Loss:{:0.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights data\n",
    "trained_weights_data_dict = train_session.get_tensors_data(train_variables.tensors)\n",
    "train_session.device.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9787b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = 1\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream((test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        facts, graph = Net().create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        outputs, = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "        \n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test program and test session\n",
    "test_session, test_input_streams, test_variables, out_stream = test_program(test_batch_size,device)\n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(dict(zip(test_variables.tensors,trained_weights_data_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a448e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(test_data, total=nr_batches):\n",
    "        inputs : Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(zip(test_input_streams,\n",
    "                                                                        [data.squeeze().float(),\n",
    "                                                                         labels.int()]))\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "print(\"Accuracy on test set: {:0.2f}%\".format(sum_acc / len(test_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
