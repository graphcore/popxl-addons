{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e686e7",
   "metadata": {},
   "source": [
    "# popxl.addons\n",
    "\n",
    "popXL allows users to hand-craft arbitrary computational graphs, controlling execution and optimizations. \n",
    "For an introduction to popXL, see the official [documentation](https://docs.sourcevertex.net/files/popart-popart-user-guide-latest/popartir.html). \n",
    "\n",
    "We assume you are familiar with the basic popxl concepts which are explained in [popXL user guide]():\n",
    "\n",
    "\t\t- IR\n",
    "\t\t- Tensors and variable tensors\n",
    "\t\t- graphs & subgraphs\n",
    "\t\t- input & output streams\n",
    "\t\t- Session\n",
    "\n",
    "We also assume you have followed [popXL mnist tutorial]()\n",
    "\n",
    "```popxl.addons``` includes common usage patterns of popXL, simplifying the process of building and training a model while keeping high control on execution and optimization. \n",
    "\n",
    "The basic steps needed to build and run a model in popxl.addons are the following:\n",
    "\n",
    "1. subclass ```addons.Module``` to create your layers and models in a object-oriented fashion. \n",
    "2. initialize an ir which represents your full compiled program and set the ```ir.replication_factor```\n",
    "3. in the ```ir.main_graph()``` context, generate the computational graph and the variable factories associated to your module with ```Module``` ```create_graph``` method\n",
    "4. in the ```ir.main_graph()``` context, instantiate actual variables with ```NamedVariableFactory``` ```init``` method.\n",
    "5. in the ```ir.main_graph()``` context, bind the computational graph to the variables using the ```bind``` method of ``GraphWithNamedArgs``` \n",
    "6. in the ```ir.main_graph()``` context,  call the bound graph providing only the inputs\n",
    "7. Specify the properties of the ir and create a [session](link to session section) to run your ir.\n",
    "8. Run the program\n",
    "\n",
    "```python\n",
    "# subclass ```addons.Module``` to create your layers and models in a object-oriented fashion.\n",
    "class myModule(addons.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        ...\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor: \n",
    "        ...\n",
    "# initialize an ir which represents your full compiled program and set replication factor\n",
    "ir = popxl.Ir()\n",
    "ir.replication_factor = 1\n",
    "with popxl.main_graph():\n",
    "    # generate the computational graph and the variable factories\n",
    "    facts, graph = myModule.create_graph(inputs_tensor_specs) \n",
    "    # instantiate actual variables\n",
    "    variables = facts.init()\n",
    "    # bind the computational graph to the variables\n",
    "    bound_graph = graph.bind(variables) \n",
    "    # call the bound graph providing only the inputs\n",
    "    outputs = bound_graph.call(inputs) \n",
    "    # the same graph can be bound to different variables,\n",
    "    # generating a new bound graph  \n",
    "    variables2 = facts.init()\n",
    "    bound_graph2 = graph.bind(variables) \n",
    "\n",
    "# Specify the properties of the ir\n",
    "ir.num_host_transfers = 1\n",
    "# Create a session to run your ir\n",
    "with popxl.Session(ir,'ipu_hw') as session:\n",
    "    # Run the program\n",
    "    session.run(inputs)\n",
    "```\n",
    "\n",
    "In this notebook we will go through these steps in depth. We are going to create a simple linear model and train it on Mnist dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a870d1c",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"images/workflow.jpg\" style=\"width:700px;\"/>\n",
    "    <figcaption> <b>Fig 1: </b> Workflow in popxl.addons \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c423920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aedd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "from typing import Mapping, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "import popxl\n",
    "import popxl_addons as addons\n",
    "import popxl.ops as ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62deb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torchvision\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb38cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c98c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40102e",
   "metadata": {},
   "source": [
    "# Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fccd5",
   "metadata": {},
   "source": [
    "To create layers and models you can inherit from the ```addons.Module``` class. It allows you to build a **graph** of the computational graph with its own **state**, i.e. with internal parameters (such as weights). \n",
    "It is similar to a pytorch Module or a Keras Layer.\n",
    "\n",
    "Making graphs is essential to enable **code reuse**, since you can assemble your program inserting ```call``` operations to a subgraph in several places instead of duplicating the nodes at each call site.\n",
    "\n",
    "```python\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\"weight\", partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "                                  x.dtype)\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "        return y\n",
    "```\n",
    "Each module implements a ```build(self, x: popxl.Tensor) -> popxl.Tensor``` method where the graph is defined.\n",
    "There are two kind of inputs, those provided as arguments of the ```build``` method (```x``` here) and **named inputs**, added via ```add_variable_input``` method (```w``` and ```b``` here).  \n",
    "\n",
    "Named inputs are the state variables of the module, its internal parameters. \n",
    "Remember that in popxl variables can only live in the main graph. Hence, state tensor variables can't be instatiated directly in the graph. Their creation needs to take place in the main graph. The ```add_variable_input``` creates a named local placeholder which will be bound to a variable living in the main graph.\n",
    "\n",
    "The ```module.create_graph``` method creates the actual graph (a **GraphWithNamedArgs**) and **VariableFactories** for the named inputs. It requires a ```TensorSpec``` or a ```Tensor``` for each of the unnamed inputs (```x```).\n",
    "\n",
    "Once you have the factories you can instantiate actual variables for each of the named inputs by calling ```variables = factories.init()``` **in the main graph**.\n",
    "\n",
    "Then, you can bind your graph to these variables with ```bound_graph = linear_graph.bind(variables)```.\n",
    "\n",
    "The resulting **BoundGraph** is effectively a graph with an internal state, which can be called with the familiar api:\n",
    "```python\n",
    "outputs = bound_graph.call(x)\n",
    "```\n",
    "The same graph can be bound to different set of parameters, resulting in different bound graphs :\n",
    "```python\n",
    "variables2 = factories.init()\n",
    "bound_graph2 = linear_graph.bind(variables)\n",
    "```\n",
    ". However, both bound graphs refer to the same computational graph, hence code is effectively reused. \n",
    "\n",
    "What actually happens is that the\n",
    "```\n",
    "bound_graph.call(input_tensor)\n",
    "```\n",
    "becomes a call operation in the main graph with the following arguments\n",
    "```\n",
    "call(graph, x, inputs_dict=variables_dict)\n",
    "```\n",
    "Calling ```bound_graph2```, which is the same graph but bound to different paramaters, results in\n",
    "```\n",
    "call(graph, x, inputs_dict=variables_dict2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085127a9",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"images/module_graph_bgraph.jpg\" style=\"width:700px;\"/>\n",
    "    <figcaption> <b>Fig 2: </b> <code>module.create_graph</code> method generates a graph and named variable factories.\n",
    "        calling <code>.init()</code> on the variable factories instantiates actual variable tensors in the main graph. \n",
    "        The same graph can be bound to different set of variables, generating different bound graphs.\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\"weight\", partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "                                  x.dtype)\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Basic example illustrating how to create a graph from a module and the concepts of factories and bound graph.\n",
    "'''\n",
    "ir = popxl.Ir()\n",
    "main = ir.main_graph\n",
    "with main:\n",
    "    # create a graph from the module and named inputs factories for each of the named inputs\n",
    "    facts, linear_graph = Linear(32).create_graph(popxl.TensorSpec((2,4), popxl.float32))\n",
    "    print(\"factories: \\n\", facts)\n",
    "    print(\"\\n graph: \\n\", linear_graph.print_schedule()) \n",
    "    # since we are in the main graph, we can instantiate variables using the factories\n",
    "    variables = facts.init()\n",
    "    variables2 = facts.init()\n",
    "    print(\"\\n variables: \\n\",variables)\n",
    "    # and bind our graph to these variables that live in the main graph. \n",
    "    bound_graph = linear_graph.bind(variables)\n",
    "    bound_graph2 = linear_graph.bind(variables2)\n",
    "    print(\"\\n two different bound graphs: \\n\", bound_graph, bound_graph2)\n",
    "\n",
    "    # the bound graph can be called providing only unnamed inputs x\n",
    "    input_data = np.asarray(np.random.rand(2,4)).astype(np.float32)\n",
    "    input_tensor = popxl.variable(input_data, name=\"x\")\n",
    "\n",
    "    out = bound_graph.call(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17442f1f",
   "metadata": {},
   "source": [
    "# Nested Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff2740",
   "metadata": {},
   "source": [
    "Modules can be combined and nicely nested. Now that we have a Linear layer, we can assemble a simple linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b66739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512)\n",
    "        self.fc2 = Linear(512)\n",
    "        self.fc3 = Linear(512)\n",
    "        self.fc4 = Linear(10)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "        x = ops.gelu(self.fc2(x))\n",
    "        x = ops.gelu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = popxl.Ir()\n",
    "main = ir.main_graph\n",
    "with main:\n",
    "    facts, net_graph = Net().create_graph(popxl.TensorSpec((28,28), popxl.float32))\n",
    "    print(facts.to_dict().keys())\n",
    "    print('\\n',net_graph.print_schedule())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e7662",
   "metadata": {},
   "source": [
    "You can see from the ```print_schedule()``` output that nested modules result in **inlined** code: the nodes are repeated for each layer, even if they are identical. For example here ```fc2``` and ```fc3``` have the exact same graph\n",
    "```\n",
    "    MatMul.104 (%13 [(1, 512) float32], %4 [(512, 512) float32]) -> (%14 [(1, 512) float32])\n",
    "    Add.105 (%14 [(1, 512) float32], %5 [(512,) float32]) -> (%15 [(1, 512) float32])\n",
    "```\n",
    "\n",
    "If you want to achieve a better code reuse you can manually **outline** the model by explicitly inserting call operations to the shared graph.\n",
    "To outline a graph you need to:\n",
    "\n",
    "    - create the graph you want to share ```factories, subgraph= Linear(512).create_graph(x)```\n",
    "    - generate different named input tensors (different local placeholders) for each of the duplicated layers. You can use the ```module.add_variable_inputs(name, factories)``` method. In this way you create different **local** tensors.\n",
    "    - bind the graph to the local tensors\n",
    "    - add call operations to the bound graphs\n",
    "    \n",
    "To outline a subgraph you need to generate different named input tensors (different local placeholders) for each of the duplicated layers. In this example, we need different variables for ```fc2``` and ```fc3```. \n",
    "The ```module.add_variable_inputs(name, factories)``` method can be used for this purpose. It produces different local named tensors (```named_tensors_0```and ```named_tensors_1``` in the example below) that you can use to create two different bound graph, each bound to its own set of variables. Once you have the bound graphs you can call them.\n",
    "\n",
    "When you call ``` facories.init()``` in the main context you generate variables for all the local tensors, including ```named_tensors_0``` and ```named_tensors_1```.  \n",
    "When you finally bind the graph, the local tensors are bound to the main variables. Since the subgraph is bound to the local tensors, it is effectively bound to them too.\n",
    "\n",
    "Below is an outlined version of the network. You can see in the graph that ```fc2``` and ```fc3``` blocks have been replaced by ```call``` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd155f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetOutlined(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        #first and last layer are not reused\n",
    "        self.fc1 = Linear(512) \n",
    "        self.fc4 = Linear(10)\n",
    "        \n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "        \n",
    "        #create a single subgraph to be used both for fc2 and fc3 \n",
    "        facts, subgraph = Linear(512).create_graph(x) #create variable factories and subgraph\n",
    "        named_tensors_0 = self.add_variable_inputs(\"fc2\", facts) # generate specific named inputs for fc2\n",
    "        fc2 = subgraph.bind(named_tensors_0) #fc2 is a bound graph using the shared, single subgraph and custom params\n",
    "        named_tensors_1 = self.add_variable_inputs(\"fc3\", facts) # generate specific named inputs for fc3\n",
    "        fc3 = subgraph.bind(named_tensors_1) #fc3 is a bound graph using the shared, single subgraph and custom params\n",
    "\n",
    "        x, = fc2.call(x)\n",
    "        x = ops.gelu(x)\n",
    "        x, = fc3.call(x)\n",
    "        x = ops.gelu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469de377",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = popxl.Ir()\n",
    "with ir.main_graph:\n",
    "    args, net_graph = NetOutlined().create_graph(popxl.TensorSpec((28,28), popxl.float32))\n",
    "    print(args.to_dict().keys())\n",
    "    print('\\n',net_graph.print_schedule())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae5a1b",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(test_batch_size : int, batch_size : int ):\n",
    "    training_data = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "        '~/.torch/datasets',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307, ), (0.3081, )), # mean and std computed on the training set.\n",
    "            ])),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True)\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "        '~/.torch/datasets',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307, ), (0.3081, )),\n",
    "            ]\n",
    "        )),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True)\n",
    "    return training_data, validation_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e155f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02986ef6",
   "metadata": {},
   "source": [
    "Now that we have a dataset and a model we are ready to construct the training program.\n",
    "\n",
    "First of all we initialise the [ir](https://docs.sourcevertex.net/files/popart-popart-ir-user-guide-latest/popartir_concepts.html#irs) for the training program. We also need to specify the ```replication_factor``` for the ir at the moment of initialisation, before constructing the program, because some operations need to know the number of ipus involved.\n",
    "\n",
    "Inside the main graph context of the ir, \n",
    "we construct [streams](https://docs.sourcevertex.net/files/popart-popart-ir-user-guide-latest/popartir_op.html#data-input-and-output) to transfer input data from the host to the device (```popxl.h2d_stream```) and we load data to the device (```popxl.host_load```).\n",
    "\n",
    "Then, we create two graphs: one for the forward and one for the backward. The latter can be obtained from the forward graph applying a **transform**, which is a way of making changes at graph level. The ```addons.autodiff``` transform is the one we need. It is basically [popxl.autodiff](https://docs.sourcevertex.net/files/popart-popart-ir-user-guide-latest/popartir_transforms.html) with some additional patterns.\n",
    "\n",
    "We instantiate the weights of the network (```variables = facts.init()```), bind the forward graph to these variables and make the call to the forward graph.\n",
    "We use ```call_with_info``` because we want to be able to retrieve the activations from the forward and pass them to the backward (see [calling-a-subgraph](https://docs.sourcevertex.net/files/popart-popart-ir-user-guide-latest/popartir_graph.html#calling-a-subgraph))\n",
    "\n",
    "We also add the loss with:\n",
    "```\n",
    "loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "```\n",
    "This operation returns the loss tensor and the gradient to start backpropagation, which is 1 (dl/dl) unless you specify the additional argument ```loss_scaling```.\n",
    "\n",
    "We create an output stream from the device to the host in order to retrieve loss values.\n",
    "\n",
    "Finally, we call the backward graph and retrieve a dictionary between each tensor in the forward and its corresponding gradient in the backward with\n",
    "```python\n",
    " grads = bwd_graph.grad_graph_info.fwd_parent_ins_to_grad_parent_outs(fwd_info, bwd_info)\n",
    "```\n",
    "This dictionary can then be used by the optimizer to update the weights of the model.\n",
    "\n",
    "Finally, we setup the ```ir``` properties, specifying the ```num_host_transfers```, and we return a [Session]() so that we can execute our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951eb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "test_batch_size = 80\n",
    "device = \"ipu_hw\" \n",
    "lr = 0.05\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_program(batch_size, device, lr):\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = 1\n",
    "    with ir.main_graph:\n",
    "        # Create input streams from host to device \n",
    "        img_stream = popxl.h2d_stream((batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        img_t = ops.host_load(img_stream) #load data\n",
    "        label_stream = popxl.h2d_stream((batch_size, ), popxl.int32, \"labels\")\n",
    "        labels = ops.host_load(label_stream, \"labels\")\n",
    "\n",
    "        # Create forward graph\n",
    "        facts, fwd_graph = Net().create_graph(img_t)\n",
    "        # Create backward graph via autodiff transform\n",
    "        bwd_graph = addons.autodiff(fwd_graph)\n",
    "\n",
    "        # Initialise variables (weights)\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Call the forward with call_with_info because we want to retrieve information from the call site\n",
    "        fwd_info = fwd_graph.bind(variables).call_with_info(img_t)\n",
    "        x = fwd_info.outputs[0] # forward output\n",
    "        \n",
    "        # Compute loss and starting gradient for backprop \n",
    "        loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "        \n",
    "        # Setup a stream to retrieve loss values from the host\n",
    "        loss_stream = popxl.d2h_stream(loss.shape, loss.dtype, \"loss\")\n",
    "        ops.host_store(loss_stream, loss)\n",
    "        \n",
    "        # retrieve activations from the forward\n",
    "        activations = bwd_graph.grad_graph_info.inputs_dict(fwd_info)\n",
    "        # call the backward providing the starting value for backprop and activations\n",
    "        bwd_info = bwd_graph.call_with_info(dx, args=activations)\n",
    "        \n",
    "        # Optimizer: get a dictionary between forward tensors and corresponding gradients and use it to update\n",
    "        # each tensor\n",
    "        grads_dict = bwd_graph.grad_graph_info.fwd_parent_ins_to_grad_parent_outs(fwd_info, bwd_info)\n",
    "        for t in variables.tensors:\n",
    "            ops.scaled_add_(t, grads_dict[t], b=-lr)\n",
    "            \n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir,device), [img_stream, label_stream], variables, loss_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9323e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = get_mnist_data(test_batch_size, train_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(train_batch_size,device,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_batches = len(training_data)\n",
    "with train_session:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch {0}/{1}\".format(epoch, epochs))\n",
    "        bar = tqdm(training_data, total=nr_batches)\n",
    "        for data, labels in bar:\n",
    "            inputs : Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(zip(train_input_streams,\n",
    "                                                                            [data.squeeze().float(),\n",
    "                                                                            labels.int()]))\n",
    "            loss = train_session.run(inputs)[loss_stream]\n",
    "            bar.set_description(\"Loss:{:0.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ccf0d",
   "metadata": {},
   "source": [
    "After training we retrieve the trained weights to use them during inference and test the accuracy of the model.\n",
    "To do that, we need to get the data stored in the tensor on the device with ```session.get_tensors_data(tensors)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9972b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights data\n",
    "trained_weights_data_dict = train_session.get_tensors_data(train_variables.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddb480",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d3af2",
   "metadata": {},
   "source": [
    "To test our model we need to create an inference-only program and run it on the test dataset. \n",
    "Again, we initialise the ir.\n",
    "Then, we define the input stream, the graph and the factories for the variables. We instantiate the variables and bind the graph to them, obtaining our bound graph. Finally, we call the model. Since this time we don't need to retrieve information from the call site, we can simply use ```call``` instead of ```call_with_info```. We store the output in an output stream for later use.\n",
    "\n",
    "One piece is still missing: we need to initialise the model variables with the weights we have just trained.\n",
    "To do that, once the test_session is created we can call ```test_session.write_variables_data```. This function requires a dictionary ``` tensor_to_be_written : tensor_data_to_write ```. It makes a call to ```test_session.write_variable_data(tensor,tensor_data)``` for each tensor in the dictionary and then makes a single host to device transfer at the end to send all data in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate accuracy from the predictions of the model.\n",
    "Predictions do not need to be normalized.\n",
    "'''\n",
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = 1\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream((test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        facts, graph = Net().create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        outputs, = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "        \n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test program and test session\n",
    "test_session, test_input_streams, test_variables, out_stream = test_program(test_batch_size,device)\n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(dict(zip(test_variables.tensors,trained_weights_data_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that weights have been copied correctly\n",
    "weights_data_dict = test_session.get_tensors_data(test_variables.tensors)\n",
    "names_to_data_dic_train = dict(zip(train_variables.named_tensors.keys(),trained_weights_data_dict.values()))\n",
    "names_to_data_dic_test =  dict(zip(test_variables.named_tensors.keys(),weights_data_dict.values()))\n",
    "for name, array in names_to_data_dic_test.items():\n",
    "    assert ( array == names_to_data_dic_train[name] ).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with torch.no_grad(), test_session:\n",
    "    for data, labels in tqdm(test_data, total=nr_batches):\n",
    "        inputs : Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(zip(test_input_streams,\n",
    "                                                                        [data.squeeze().float(),\n",
    "                                                                         labels.int()]))\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "print(\"Accuracy on test set: {:0.2f}%\".format(sum_acc / len(test_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
