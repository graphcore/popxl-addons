{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98dcda5c",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d95e91",
   "metadata": {},
   "source": [
    "# popxl.addons\n",
    "`popXL` is an experimental PopART Python module which allows users to hand-craft arbitrary computational graphs, controlling execution and optimisations.\n",
    "For an introduction to `popXL`, see the official [documentation](https://docs.graphcore.ai/projects/popxl/en/latest/).\n",
    "\n",
    "We assume you are familiar with the basic `popXL` concepts which are explained in [popXL user guide](https://docs.graphcore.ai/projects/popxl/en/latest/):\n",
    "- Intermediate representation (IR)\n",
    "- Tensors (variable, constant and intermediate)\n",
    "- Graphs (main, parent and subgraphs)\n",
    "- Input and output streams\n",
    "- Sessions\n",
    "\n",
    "We also assume you have completed the [popXL mnist tutorial]().\n",
    "\n",
    "`popxl.addons` includes common usage patterns of `popXL`, simplifying the process of building and training a model while keeping high control on execution and optimization.\n",
    "\n",
    "The basic steps needed to build and run a model in `popxl.addons` are the following:\n",
    "\n",
    "1. Subclass `addons.Module` to create your layers and models in an object-oriented fashion.\n",
    "2. Initialise an IR as ``ir`` which represents your full compiled program and set the `ir.replication_factor`\n",
    "3. In the `ir.main_graph()` context, generate the computational graph and the variable factories associated with your module with the `Module` `create_graph` method\n",
    "4. In the `ir.main_graph()` context, instantiate actual variables with the `NamedVariableFactory` `init` method.\n",
    "5. In the `ir.main_graph()` context, bind the computational graph to the variables using the `bind` method of ``GraphWithNamedArgs`\n",
    "6. In the `ir.main_graph()` context,  call the bound graph providing only the inputs.\n",
    "7. Specify the properties of the IR and create a [session](link to session section) to run your IR.\n",
    "8. Run the program.\n",
    "\n",
    "```python\n",
    "# subclass addons.Module to create your layers and models in an object-oriented fashion.\n",
    "class myModule(addons.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        ...\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        ...\n",
    "# initialise an IR which represents your full compiled program and set replication factor\n",
    "ir = popxl.Ir()\n",
    "ir.replication_factor = 1\n",
    "with popxl.main_graph():\n",
    "    # generate the computational graph and the variable factories\n",
    "    facts, graph = myModule.create_graph(inputs_tensor_specs)\n",
    "    # instantiate actual variables\n",
    "    variables = facts.init()\n",
    "    # bind the computational graph to the variables\n",
    "    bound_graph = graph.bind(variables)\n",
    "    # call the bound graph providing only the inputs\n",
    "    outputs = bound_graph.call(inputs)\n",
    "    # the same graph can be bound to different variables,\n",
    "    # generating a new bound graph\n",
    "    variables2 = facts.init()\n",
    "    bound_graph2 = graph.bind(variables)\n",
    "\n",
    "# Specify the properties of the IR\n",
    "ir.num_host_transfers = 1\n",
    "# Create a session to run your IR\n",
    "session = popxl.Session(ir,'ipu_hw')\n",
    "# Run the program\n",
    "session.run(inputs)\n",
    "```\n",
    "\n",
    "In this notebook we will go through these steps in detail. We are going to create a simple linear model and train it on the MNIST dataset.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"images/workflow.png\"/>\n",
    "    <figcaption> <b>Fig 1: </b> Workflow in popxl.addons\n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6578796",
   "metadata": {},
   "source": [
    "# Basic concepts\n",
    "\n",
    "To create layers and models you can inherit from the `addons.Module` class.\n",
    "It allows you to build a **graph** of the computational graph with its own **state**, which means with internal parameters (such as weights).\n",
    "It is similar to a PyTorch Module or a Keras Layer.\n",
    "\n",
    "Creating graphs is essential to enable **code reuse**, since you can assemble your program by inserting `call` operations into a subgraph in several places instead of duplicating the nodes at each call site.\n",
    "\n",
    "```python\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\"weight\", partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "                                  x.dtype)\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "        return y\n",
    "```\n",
    "Each module implements a `build(self, x: popxl.Tensor) -> popxl.Tensor` method where the graph is defined.\n",
    "There are two kind of inputs, those provided as arguments of the `build` method (`x` here) and **named inputs**, added via the `add_variable_input` method (`w` and `b` here).\n",
    "\n",
    "Named inputs are the state variables of the module, its internal parameters.\n",
    "Remember that in `popXL`, variables can only live in the main graph.\n",
    "Hence, state tensor variables cannot be instatiated directly in the graph. Their creation needs to take place in the main graph.\n",
    "The `add_variable_input` method creates a named local placeholder which will be bound to a variable living in the main graph.\n",
    "\n",
    "The `module.create_graph` method creates the actual graph (a **GraphWithNamedArgs**) and **VariableFactories** for the named inputs.\n",
    "It requires a `TensorSpec` object or a `Tensor` object for each of the unnamed inputs (`x`).\n",
    "\n",
    "Once you have the factories you can instantiate actual variables for each of the named inputs by calling `variables = factories.init()` **in the main graph**.\n",
    "\n",
    "Then, you can bind your graph to these variables with `bound_graph = linear_graph.bind(variables)`.\n",
    "\n",
    "The resulting **BoundGraph** is effectively a graph with an internal state, which can be called with:\n",
    "```python\n",
    "outputs = bound_graph.call(x)\n",
    "```\n",
    "The same graph can be bound to different sets of parameters, resulting in different bound graphs :\n",
    "```python\n",
    "variables2 = factories.init()\n",
    "bound_graph2 = linear_graph.bind(variables)\n",
    "```\n",
    "However, both bound graphs refer to the same computational graph, hence code is effectively reused.\n",
    "\n",
    "What actually happens is that\n",
    "```python\n",
    "bound_graph.call(input_tensor)\n",
    "```\n",
    "becomes a call operation in the main graph with the following arguments:\n",
    "```python\n",
    "call(graph, x, inputs_dict=variables_dict)\n",
    "```\n",
    "Calling ```bound_graph2```, which is the same graph but bound to different paramaters, results in\n",
    "```python\n",
    "call(graph, x, inputs_dict=variables_dict2)\n",
    "```\n",
    "\n",
    "<figure>\n",
    "    <img src=\"images/module_graph_bgraph.png\"/>\n",
    "    <figcaption> <b>Fig 2: </b>  The <code>module.create_graph</code> method generates a graph and named variable factories.\n",
    "        Calling <code>.init()</code> on the variable factories instantiates actual variable tensors in the main graph.\n",
    "        The same graph can be bound to different sets of variables, generating different bound graphs.\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29191c86",
   "metadata": {},
   "source": [
    "We now import the necessary packages, including `popxl` and the `popxl.addons`, and write a basic example to illustrate these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b64ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:48:20.707505Z",
     "iopub.status.busy": "2022-05-03T14:48:20.706401Z",
     "iopub.status.idle": "2022-05-03T14:48:22.085877Z",
     "shell.execute_reply": "2022-05-03T14:48:22.086710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factories: \n",
      " {'weight': <popxl_addons.variable_factory.VariableFactory object at 0x7f064d9f17b8>, 'bias': <popxl_addons.variable_factory.VariableFactory object at 0x7f064dc3ca90>}\n",
      "\n",
      " graph: \n",
      " Graph : Linear_subgraph(0)\n",
      "  (%1, weight=%2, bias=%3) -> (%5) {\n",
      "    MatMul.100 (%1 [(2, 4) float32], %2 [(4, 32) float32]) -> (%4 [(2, 32) float32])\n",
      "    Add.101 (%4 [(2, 32) float32], %3 [(32,) float32]) -> (%5 [(2, 32) float32])\n",
      "  }\n",
      "\n",
      " variables: \n",
      " {'bias': Tensor[bias popxl.dtypes.float32 (32,)], 'weight': Tensor[weight popxl.dtypes.float32 (4, 32)]}\n",
      "\n",
      " two different bound graphs: \n",
      " <popxl_addons.graph.BoundGraph object at 0x7f064d9f17f0> <popxl_addons.graph.BoundGraph object at 0x7f064d9f1be0>\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Mapping, Optional\n",
    "from functools import partial\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import popxl.ops as ops\n",
    "import popxl_addons as addons\n",
    "import popxl\n",
    "\n",
    "\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\"weight\", partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "                                    x.dtype)\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\n",
    "                \"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "        return y\n",
    "\n",
    "\n",
    "ir = popxl.Ir()\n",
    "main = ir.main_graph\n",
    "with main:\n",
    "    # create a graph from the module and named variable factories for each of the named inputs\n",
    "    facts, linear_graph = Linear(32).create_graph(\n",
    "        popxl.TensorSpec((2, 4), popxl.float32))\n",
    "    print(\"factories: \\n\", facts)\n",
    "    print(\"\\n graph: \\n\", linear_graph.print_schedule())\n",
    "    # since we are in the main graph, we can instantiate variables using the factories\n",
    "    variables = facts.init()\n",
    "    variables2 = facts.init()\n",
    "    print(\"\\n variables: \\n\", variables)\n",
    "    # and bind our graph to these variables that live in the main graph.\n",
    "    bound_graph = linear_graph.bind(variables)\n",
    "    bound_graph2 = linear_graph.bind(variables2)\n",
    "    print(\"\\n two different bound graphs: \\n\", bound_graph, bound_graph2)\n",
    "\n",
    "    # the bound graph can be called providing only unnamed inputs x\n",
    "    input_data = np.asarray(np.random.rand(2, 4)).astype(np.float32)\n",
    "    input_tensor = popxl.variable(input_data, name=\"x\")\n",
    "\n",
    "    out = bound_graph.call(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63144d0",
   "metadata": {},
   "source": [
    "# Nested Modules\n",
    "\n",
    "Modules can be combined and easily nested. Now that we have a `Linear` layer, we can assemble a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dca6058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:48:22.096292Z",
     "iopub.status.busy": "2022-05-03T14:48:22.095609Z",
     "iopub.status.idle": "2022-05-03T14:48:22.287273Z",
     "shell.execute_reply": "2022-05-03T14:48:22.286408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n",
      "\n",
      " Graph : Net_subgraph(0)\n",
      "  (%1, fc1.weight=%2, fc1.bias=%3, fc2.weight=%4, fc2.bias=%5, fc3.weight=%6, fc3.bias=%7, fc4.weight=%8, fc4.bias=%9) -> (%21) {\n",
      "    Reshape.100 (%1 [(28, 28) float32]) -> (%10 [(1, 784) float32])\n",
      "    MatMul.101 (%10 [(1, 784) float32], %2 [(784, 512) float32]) -> (%11 [(1, 512) float32])\n",
      "    Add.102 (%11 [(1, 512) float32], %3 [(512,) float32]) -> (%12 [(1, 512) float32])\n",
      "    Gelu.103 (%12 [(1, 512) float32]) -> (%13 [(1, 512) float32])\n",
      "    MatMul.104 (%13 [(1, 512) float32], %4 [(512, 512) float32]) -> (%14 [(1, 512) float32])\n",
      "    Add.105 (%14 [(1, 512) float32], %5 [(512,) float32]) -> (%15 [(1, 512) float32])\n",
      "    Gelu.106 (%15 [(1, 512) float32]) -> (%16 [(1, 512) float32])\n",
      "    MatMul.107 (%16 [(1, 512) float32], %6 [(512, 512) float32]) -> (%17 [(1, 512) float32])\n",
      "    Add.108 (%17 [(1, 512) float32], %7 [(512,) float32]) -> (%18 [(1, 512) float32])\n",
      "    Gelu.109 (%18 [(1, 512) float32]) -> (%19 [(1, 512) float32])\n",
      "    MatMul.110 (%19 [(1, 512) float32], %8 [(512, 10) float32]) -> (%20 [(1, 10) float32])\n",
      "    Add.111 (%20 [(1, 10) float32], %9 [(10,) float32]) -> (%21 [(1, 10) float32])\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "class Net(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512)\n",
    "        self.fc2 = Linear(512)\n",
    "        self.fc3 = Linear(512)\n",
    "        self.fc4 = Linear(10)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "        x = ops.gelu(self.fc2(x))\n",
    "        x = ops.gelu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "ir = popxl.Ir()\n",
    "main = ir.main_graph\n",
    "with main:\n",
    "    facts, net_graph = Net().create_graph(popxl.TensorSpec((28, 28), popxl.float32))\n",
    "    print(facts.to_dict().keys())\n",
    "    print('\\n', net_graph.print_schedule())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedea44d",
   "metadata": {},
   "source": [
    "You can see from the `print_schedule()` output that nested modules result in **inlined** code: the nodes are repeated for each layer, even if they are identical.\n",
    "For example here `fc2` and `fc3` have the exact same graph.\n",
    "\n",
    "If you want to achieve better code reuse you can manually **outline** the model by explicitly inserting call operations into the shared graph.\n",
    "To outline a graph you need to:\n",
    "- create the graph you want to share: `factories, subgraph= Linear(512).create_graph(x)`\n",
    "- generate different named input tensors (different local placeholders) for each of the duplicated layers. You can use the `module.add_variable_inputs(name, factories)` method. In this way you create different **local** tensors.\n",
    "- bind the graph to the local tensors\n",
    "- add call operations to the bound graphs\n",
    "\n",
    "To outline a subgraph you need to generate different named input tensors (different local placeholders) for each of the duplicated layers.\n",
    "In this example, we need different variables for `fc2` and `fc3`.\n",
    "The `module.add_variable_inputs(name, factories)` method can be used for this purpose.\n",
    "It produces different local named tensors (`named_tensors_0`and `named_tensors_1` in the example below) that you can use to create two different bound graphs, each bound to its own set of variables.\n",
    "Once you have the bound graphs you can call each of them.\n",
    "\n",
    "When you call `facories.init()` in the main context you generate variables for all the local tensors, including `named_tensors_0` and `named_tensors_1`.\n",
    "When you finally bind the graph, the local tensors are bound to the main variables. Since the subgraph is bound to the local tensors, it is effectively bound to them too.\n",
    "\n",
    "Below is an outlined version of the network. You can see in the graph that the `fc2` and `fc3` blocks have been replaced by `call` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37bcc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:48:22.297318Z",
     "iopub.status.busy": "2022-05-03T14:48:22.296414Z",
     "iopub.status.idle": "2022-05-03T14:48:22.456380Z",
     "shell.execute_reply": "2022-05-03T14:48:22.457085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1.weight', 'fc1.bias', 'fc4.weight', 'fc4.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "\n",
      " Graph : NetOutlined_subgraph(0)\n",
      "  (%1, fc1.weight=%2, fc1.bias=%3, fc2.weight=%4, fc2.bias=%5, fc3.weight=%6, fc3.bias=%7, fc4.weight=%8, fc4.bias=%9) -> (%19) {\n",
      "    Reshape.100 (%1 [(28, 28) float32]) -> (%10 [(1, 784) float32])\n",
      "    MatMul.101 (%10 [(1, 784) float32], %2 [(784, 512) float32]) -> (%11 [(1, 512) float32])\n",
      "    Add.102 (%11 [(1, 512) float32], %3 [(512,) float32]) -> (%12 [(1, 512) float32])\n",
      "    Gelu.103 (%12 [(1, 512) float32]) -> (%13 [(1, 512) float32])\n",
      "    Call.106(Linear_subgraph(1)) (%13 [(1, 512) float32], %4 [(512, 512) float32], %5 [(512,) float32]) -> (%14 [(1, 512) float32])\n",
      "    Gelu.107 (%14 [(1, 512) float32]) -> (%15 [(1, 512) float32])\n",
      "    Call.108(Linear_subgraph(1)) (%15 [(1, 512) float32], %6 [(512, 512) float32], %7 [(512,) float32]) -> (%16 [(1, 512) float32])\n",
      "    Gelu.109 (%16 [(1, 512) float32]) -> (%17 [(1, 512) float32])\n",
      "    MatMul.110 (%17 [(1, 512) float32], %8 [(512, 10) float32]) -> (%18 [(1, 10) float32])\n",
      "    Add.111 (%18 [(1, 10) float32], %9 [(10,) float32]) -> (%19 [(1, 10) float32])\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "class NetOutlined(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        # first and last layer are not reused\n",
    "        self.fc1 = Linear(512)\n",
    "        self.fc4 = Linear(10)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "\n",
    "        # create a single subgraph to be used both for fc2 and fc3\n",
    "        # create variable factories and subgraph\n",
    "        facts, subgraph = Linear(512).create_graph(x)\n",
    "        # generate specific named inputs for fc2\n",
    "        named_tensors_0 = self.add_variable_inputs(\"fc2\", facts)\n",
    "        # fc2 is a bound graph using the shared, single subgraph and custom params\n",
    "        fc2 = subgraph.bind(named_tensors_0)\n",
    "        # generate specific named inputs for fc3\n",
    "        named_tensors_1 = self.add_variable_inputs(\"fc3\", facts)\n",
    "        # fc3 is a bound graph using the shared, single subgraph and custom params\n",
    "        fc3 = subgraph.bind(named_tensors_1)\n",
    "\n",
    "        x, = fc2.call(x)\n",
    "        x = ops.gelu(x)\n",
    "        x, = fc3.call(x)\n",
    "        x = ops.gelu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "ir = popxl.Ir()\n",
    "with ir.main_graph:\n",
    "    args, net_graph = NetOutlined().create_graph(\n",
    "        popxl.TensorSpec((28, 28), popxl.float32))\n",
    "    print(args.to_dict().keys())\n",
    "    print('\\n', net_graph.print_schedule())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55bf76",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "We are now ready to build the full program to train and validate a linear model on the MNIST dataset.\n",
    "\n",
    "First of all, we need to load the dataset. We are going to use Pytorch DataLoader.\n",
    "Data is normalised using the mean and std deviation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225ff72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:48:22.463978Z",
     "iopub.status.busy": "2022-05-03T14:48:22.463033Z",
     "iopub.status.idle": "2022-05-03T14:48:22.465808Z",
     "shell.execute_reply": "2022-05-03T14:48:22.465076Z"
    },
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "def get_mnist_data(test_batch_size: int, batch_size: int):\n",
    "    training_data = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "        '~/.torch/datasets',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                # mean and std computed on the training set.\n",
    "                torchvision.transforms.Normalize((0.1307, ), (0.3081, )),\n",
    "            ])),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "        '~/.torch/datasets',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307, ), (0.3081, )),\n",
    "            ]\n",
    "        )),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True)\n",
    "    return training_data, validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b65fd3",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now that we have a dataset and a model we are ready to construct the training program.\n",
    "\n",
    "First of all we initialise the [IR](https://docs.graphcore.ai/projects/popxl/en/latest/concepts.html#irs) for the training program.\n",
    "We also need to specify the `replication_factor` for the IR at the moment of initialisation, before constructing the program, because some operations need to know\n",
    "the number of IPUs involved.\n",
    "\n",
    "Inside the main graph context of the IR,\n",
    "we construct [streams](https://docs.graphcore.ai/projects/popxl/en/latest/op.html#data-input-and-output) to transfer input data from the host to the device\n",
    "(`popxl.h2d_stream`) and we load data to the device (`popxl.host_load`).\n",
    "\n",
    "Then, we create two graphs: one for the forward pass and one for the backward pass.\n",
    "The latter can be obtained from the forward graph applying a **transform**, which is a way of making changes at the graph level.\n",
    "The `addons.autodiff` transform is the one we need.\n",
    "It is basically [popxl.autodiff](https://docs.graphcore.ai/projects/popxl/en/latest/transforms.html#autodiff) with some additional patterns.\n",
    "\n",
    "We instantiate the weights of the network (`variables = facts.init()`), bind the forward graph to these variables and make the call to the forward graph.\n",
    "We use `call_with_info` because we want to be able to retrieve the activations from the forward graph and pass them to the backward graph (see [calling-a-subgraph](https://docs.graphcore.ai/projects/popxl/en/latest/graph.html#calling-a-subgraph))\n",
    "\n",
    "We also add the loss with:\n",
    "```python\n",
    "loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "```\n",
    "This operation returns the loss tensor and the gradient to start backpropagation, which is 1 (dl/dl) unless you specify the additional argument `loss_scaling`.\n",
    "\n",
    "We create an output stream from the device to the host in order to retrieve loss values.\n",
    "\n",
    "Finally, we call the backward graph and retrieve a dictionary mapping each tensor in the forward graph to its corresponding gradient in the backward graph with:\n",
    "```python\n",
    " grads = bwd_graph.grad_graph_info.fwd_parent_ins_to_grad_parent_outs(fwd_info, bwd_info)\n",
    "```\n",
    "This dictionary can then be used by the optimizer to update the weights of the model.\n",
    "\n",
    "Finally, we setup the properties for `ir`, specifying `num_host_transfers`, and we return a [Session](https://docs.graphcore.ai/projects/popxl/en/latest/session.html) so that we can execute our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a8317d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:48:22.469788Z",
     "iopub.status.busy": "2022-05-03T14:48:22.469357Z",
     "iopub.status.idle": "2022-05-03T14:48:22.472066Z",
     "shell.execute_reply": "2022-05-03T14:48:22.471713Z"
    }
   },
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "test_batch_size = 80\n",
    "device = \"ipu_hw\"\n",
    "lr = 0.05\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046b77cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:48:22.485664Z",
     "iopub.status.busy": "2022-05-03T14:48:22.485204Z",
     "iopub.status.idle": "2022-05-03T14:49:03.763664Z",
     "shell.execute_reply": "2022-05-03T14:49:03.764379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:0.0670: 100%|██████████| 7500/7500 [00:20<00:00, 359.66it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_program(batch_size, device, lr):\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = 1\n",
    "    with ir.main_graph:\n",
    "        # Create input streams from host to device\n",
    "        img_stream = popxl.h2d_stream(\n",
    "            (batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        img_t = ops.host_load(img_stream)  # load data\n",
    "        label_stream = popxl.h2d_stream((batch_size, ), popxl.int32, \"labels\")\n",
    "        labels = ops.host_load(label_stream, \"labels\")\n",
    "\n",
    "        # Create forward graph\n",
    "        facts, fwd_graph = Net().create_graph(img_t)\n",
    "        # Create backward graph via autodiff transform\n",
    "        bwd_graph = addons.autodiff(fwd_graph)\n",
    "\n",
    "        # Initialise variables (weights)\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Call the forward graph with call_with_info because we want to retrieve information from the call site\n",
    "        fwd_info = fwd_graph.bind(variables).call_with_info(img_t)\n",
    "        x = fwd_info.outputs[0]  # forward output\n",
    "\n",
    "        # Compute loss and starting gradient for backpropagation\n",
    "        loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "\n",
    "        # Setup a stream to retrieve loss values from the host\n",
    "        loss_stream = popxl.d2h_stream(loss.shape, loss.dtype, \"loss\")\n",
    "        ops.host_store(loss_stream, loss)\n",
    "\n",
    "        # retrieve activations from the forward graph\n",
    "        activations = bwd_graph.grad_graph_info.inputs_dict(fwd_info)\n",
    "        # call the backward graph providing the starting value for backpropagation and activations\n",
    "        bwd_info = bwd_graph.call_with_info(dx, args=activations)\n",
    "\n",
    "        # Optimizer: get a mapping between forward tensors and corresponding gradients and use it to update\n",
    "        # each tensor\n",
    "        grads_dict = bwd_graph.grad_graph_info.fwd_parent_ins_to_grad_parent_outs(\n",
    "            fwd_info, bwd_info)\n",
    "        for t in variables.tensors:\n",
    "            ops.scaled_add_(t, grads_dict[t], b=-lr)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [img_stream, label_stream], variables, loss_stream\n",
    "\n",
    "\n",
    "training_data, test_data = get_mnist_data(test_batch_size, train_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(\n",
    "    train_batch_size, device, lr)\n",
    "\n",
    "nr_batches = len(training_data)\n",
    "with train_session:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch {0}/{1}\".format(epoch, epochs))\n",
    "        bar = tqdm(training_data, total=nr_batches)\n",
    "        for data, labels in bar:\n",
    "            inputs : Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(zip(train_input_streams,\n",
    "                                                                            [data.squeeze().float(),\n",
    "                                                                            labels.int()]))\n",
    "            loss = train_session.run(inputs)[loss_stream]\n",
    "            bar.set_description(\"Loss:{:0.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23e1f6",
   "metadata": {},
   "source": [
    "After training we retrieve the trained weights to use them during inference and test the accuracy of the model.\n",
    "To do that, we need to get the data stored in the tensor on the device with `session.get_tensors_data(tensors)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5dfef8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:49:03.771017Z",
     "iopub.status.busy": "2022-05-03T14:49:03.770192Z",
     "iopub.status.idle": "2022-05-03T14:49:03.771920Z",
     "shell.execute_reply": "2022-05-03T14:49:03.772584Z"
    }
   },
   "outputs": [],
   "source": [
    "# get weights data : dictionary { train_session variables : tensor data (numpy) }\n",
    "train_vars_to_data = train_session.get_tensors_data(train_variables.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2689fe7",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "To test our model we need to create an inference-only program and run it on the test dataset.\n",
    "Again, we initialise the IR.\n",
    "Then, we define the input stream, the graph and the factories for the variables.\n",
    "We instantiate the variables and bind the graph to them, obtaining our bound graph.\n",
    "Finally, we call the model. Since this time we do not need to retrieve information from the call site, we can simply use `call` instead of `call_with_info`.\n",
    "We store the output in an output stream for later use.\n",
    "\n",
    "One piece is still missing: we need to initialise the model variables with the weights we have just trained.\n",
    "\n",
    "To do that, once the test_session is created we can call ```test_session.write_variables_data```. This function requires a dictionary ``` tensor_to_be_written : tensor_data_to_write ```. \n",
    "It makes a call to ```test_session.write_variable_data(tensor,tensor_data)``` for each tensor in the dictionary and then makes a single host to device transfer at the end to send all data in one go.\n",
    "In our case, `tensor_to_be_written` need to be the `test_session` variables, and `tensor_data_to_write` need to be the tensor values, given as numpy arrays.\n",
    "\n",
    "We already have a dictionary between `train_session` variables and their values, the `train_weights_data_dict` retrieved with `train_session.get_tensor_data`.\n",
    "\n",
    "We need a dictionary between `test_session` variables and `train_session` variables, so that we can create the required `test_variables : test_variables_data` dictionary.\n",
    "\n",
    "Provided test and training variables are named the same, we can use the `DotTree.to_mapping` function to create this mapping. Given two DotTrees, for common keys this function create a dictionary of their values. So, `train_variables.to_mapping(test_variables)` returns a dictionary ` popxl.Tensor : popxl.Tensor`, where each key-value pair is made of a train variable and the test variable with the same name. \n",
    "\n",
    "With these two dictionaries, we can finally build the required `test_variables : test_variables_data` dictionary.\n",
    "```python\n",
    "# get weights data : dictionary { train_session variables : tensor data (numpy) }\n",
    "train_vars_to_data = train_session.get_tensors_data(train_variables.tensors)\n",
    "\n",
    "# dictionary { train_session variables : test_session variables }\n",
    "train_vars_to_test_vars = train_variables.to_mapping(test_variables)\n",
    "\n",
    "# Create a dictionary { test_session variables : tensor data (numpy) }\n",
    "test_vars_to_data = {\n",
    "    test_var: train_vars_to_data[train_var].copy()\n",
    "    for train_var, test_var in train_vars_to_test_vars.items() \n",
    "}\n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(test_vars_to_data)\n",
    "```\n",
    "\n",
    "We call `.copy` each tensor because `get_tensors_data` returns a memory view of the data, hence they may become invalid if the session is invalidated (or they may change if we do something else later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315c7355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T14:49:03.796605Z",
     "iopub.status.busy": "2022-05-03T14:49:03.796197Z",
     "iopub.status.idle": "2022-05-03T14:49:21.101695Z",
     "shell.execute_reply": "2022-05-03T14:49:21.100903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 97.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 96.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Evaluate accuracy from the predictions of the model.\n",
    "Predictions do not need to be normalized.\n",
    "'''\n",
    "\n",
    "\n",
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0\n",
    "\n",
    "\n",
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = 1\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream(\n",
    "            (test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        facts, graph = Net().create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        outputs, = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream\n",
    "\n",
    "\n",
    "# Create test program and test session\n",
    "test_session, test_input_streams, test_variables, out_stream = test_program(test_batch_size,device)\n",
    "\n",
    "# dictionary { train_session variables : test_session variables }\n",
    "train_vars_to_test_vars = train_variables.to_mapping(test_variables)\n",
    "\n",
    "# Create a dictionary { test_session variables : tensor data (numpy) }\n",
    "test_vars_to_data = {\n",
    "    test_var: train_vars_to_data[train_var].copy()\n",
    "    for train_var, test_var in train_vars_to_test_vars.items() \n",
    "}    \n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(test_vars_to_data)\n",
    "\n",
    "# check that weights have been copied correctly\n",
    "test_vars_to_data_after_write = test_session.get_tensors_data(test_variables.tensors)\n",
    "for test_var, array in test_vars_to_data_after_write.items():\n",
    "    assert (array == test_vars_to_data[test_var]).all()\n",
    "\n",
    "nr_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with torch.no_grad(), test_session:\n",
    "    for data, labels in tqdm(test_data, total=nr_batches):\n",
    "        inputs : Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(zip(test_input_streams,\n",
    "                                                                        [data.squeeze().float(),\n",
    "                                                                         labels.int()]))\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "print(\"Accuracy on test set: {:0.2f}%\".format(sum_acc / len(test_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "traceability": {
   "sdk_version": "None",
   "source_file": "mnist.py",
   "sst_version": "0.0.5",
   "timestamp": "2022-05-03T15:48"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
